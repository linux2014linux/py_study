## 大模型理解
一切给予概率, 没有推理能力, 都是概率计算。
用户输入被转换为token序列(单词字符符号等序列), 每个token会被映射为高维向量(训练时产生的用来表示各个token之间**共现关系**的向量), 模型根据高维向量+多个矩阵+参数(权重)推算下一步应该产生的token概率分布。当产生结束token的概率已经是最高概率, 就意味着回答结束, 模型没必要再继续输出了。结束token可如下:
```txt
</s>
<eos>
<end>
```

将token变为高维向量, 才能进入数学领域, 让不同token的关系变得可计算。
比如:
```
“如果” → [0.12, -0.7, 1.3, ...]
“那么” → [0.11, -0.68, 1.29, ...]
```
变为向量后, 可以用计算相似度(点积/余弦),线性组合和非线性变换等做运算。

高维是为了更准确的表述, 因为文本包含的信息非常丰富, 比如:
```txt
语义相似
语法角色
时序关系
逻辑结构
领域差异
情绪/语气
...
```

梯度下降是在训练时调参数(不是学知识), 调整高维向量在空间中的相对关系是正确的。梯度下降因为模型是很多函数的复合, 不同层函数负责不同的尺度参数。
token 升维，是为了让语言关系可计算, 梯度下降，是为了在这个空间里把关系摆对。


## 归一化, 降维, 投影
归一化 对元素做相同的**线性处理**
降维 TODO

## 卷积
数学定义如下:
$$(f*g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau$$
f是输入信号，g是卷积核。
卷积(f∗g)(t)表示: 将函数 g 关于纵轴翻转得到 g(−τ)，再向右平移 t 个单位得到 g(t−τ)，然后与 f(τ) 相乘并积分——其结果等于两函数在该位置下的重叠区域的"加权面积"。
加权面积指的是计算在重叠f(τ)与g(t−τ)均为非零值时的t范围区间内的f(t)的面积。并不是图形重叠面积。
练习:
```txt
已知f(τ) = 2 (0≤τ≤2), g(τ) = 1 (0≤τ≤1)，求卷积结果.
```
结果为梯形。

```txt
已知f(τ) = τ (0<=τ<=2),g(τ) = 1 (0<=τ<=1),求 h(t)=(f∗g)(t).
```
第一段结果为 t^2 / 2，第二段结果为 t - 1/2，第三段为(-t^2 + 2t + 3)/2.

卷积分是一个函数对另一个函数的整体响应，相比积分关心卷积分关注相对位置、形状匹配和时间空间关系。
卷积分之所以要“翻转 + 平移”，即g(t - τ)，是因为它在回答一个问题：系统在时刻 t，过去所有输入对现在的贡献加起来是多少？翻转g(τ)是为了对齐时间，平移g(t-τ)是为了覆盖所有时间点。
卷积 = 翻转（对齐时间） + 平移（扫描所有时间点） + 积分（累积所有影响）
积分是全局累加。卷积分是局部匹配 + 全局累加。积分解决"量"，卷积分解决"关系"。
为什么你会在大模型/信号里反复看到"卷积思想"？因为：现实世界的影响，从来不是点对点的，而是"一个过程，对另一个过程的整体作用"。卷积正好刻画这个。
卷积分和互相关很像，互相关没有反转，直接平移。互相关时在寻找相似度高的地方。互相关将卷积中的g改为g(t+τ)即可。
静态卷积卷积核g不变，动态卷积卷积核g动态变化。

### 大模型与卷积思想
TODO 传统的CNN核心算子是卷积，但大模型引入注意力机制，是动态卷积。
初步理解，卷积核变化，指的是计算时的x轴范围与f的x重叠范围动态变化。不同t是x重叠范围不在固定。


## 人工问GPT和Agent的区别
* 问题 VS 任务
* 说 VS 做
* 短期(冗余)记忆 VS 长期(精选)记忆
* 人工提醒 VS 自动反馈
* 人工持续提问 VS 规划

## Agent的记忆和普通上下文的区别
本质上Agent会自动将记忆输入给大模型当作上下文，但是Agent会自己判断哪些信息值得记住，哪些可以丢掉，就像你大脑会自动过滤掉"昨天吃了什么"这种不重要的信息一样。
所以，不是简单的"保存后给大模型"，而是智能地"选择、摘要、检索、整合"，确保每次对话都能记住关键信息，又不会让模型"内存爆炸"。

## 个人开发者可做的方向
知识库 + 大模型（RAG）
用户输入 -> 规则 / 权限 / 校验 -> 大模型（不或轻训练）-> 结构化输出 -> 结果校验 / 回退
**文档 / Wiki / Excel -（人工或脚本拆分）-> 短知识块 -> 向量库(Milvus / FAISS / Pinecone) -> 用户提问(相似度搜索,向量余弦相似度) -> 问题 + 命中的 3～5 条知识 -(prompt组装)-> 大模型生成答案。**
### 知识拆分
* 一个篇幅的文档拆成多个 chunk，**一个chunk = 一个“可回答的知识点”**。
1. 中文：200～400 字。
2. 保留标题。
3. 同一条规则不要拆散。
4. 不要省略主语，尽量保证完整的结构。（超过10万需要审批 VS 报销金额超过10万元需要审批）
5. 不要用代词，尽量将代词替换为明确的业务名词。（这种情况需要审批 VS 跨部门报销需要审批）
* 给每个chunk加一些元信息，比如规则类型，业务域场景，时间范围等。一些限定性条件。
```json
{
  "id": "finance_rule_001",
  "title": "报销审批规则",
  "text": "...",
  "domain": "财务",
  "type": "审批规则",
  "version": "2024-06",
  "status": "active"
}
```
| 类型                   | 示例内容                                                               | 是否检索友好 | 原因                                                                     |
| ---------------------- | ---------------------------------------------------------------------- | ------------ | ------------------------------------------------------------------------ |
| ❌ 坏 chunk：过度合并   | 报销金额超过10万元需要审批，跨部门报销也需要审批，特殊情况需额外审批。 | 否           | 包含多个主判断，向量语义混杂，用户问题可能只命中其中一条，降低检索命中率 |
| ✅ 好 chunk：高金额报销 | 【标题】高金额报销审批规则<br>报销金额超过10万元需要审批。             | 是           | 单一主判断，语义集中，命中明确问题                                       |
| ✅ 好 chunk：跨部门报销 | 【标题】跨部门报销审批规则<br>跨部门报销需要审批。                     | 是           | 单一主判断，独立可检索，适用于不同问题                                   |
| ✅ 好 chunk：特殊情况   | 【标题】特殊情况审批规则<br>特殊情况下需要额外审批。                   | 是           | 单一主判断，独立可检索                                                   |

| 类型                   | 示例内容                   | 是否检索友好 | 原因                                   |
| ---------------------- | -------------------------- | ------------ | -------------------------------------- |
| ❌ 坏 chunk：主体不明确 | 超过10万需要审批           | 否           | 主语缺失（报销金额），检索时语义不完整 |
| ✅ 好 chunk：主体明确   | 报销金额超过10万元需要审批 | 是           | 主语明确，向量表达完整，检索友好       |

| 类型                       | 示例内容                                                     | 是否检索友好 | 原因                                          |
| -------------------------- | ------------------------------------------------------------ | ------------ | --------------------------------------------- |
| ❌ 坏 chunk：无信息增量重复 | 报销金额超过10万元需要审批。<br>报销金额超过10万元需要审批。 | 否           | 同一 chunk 内重复同一结论，无增益，浪费 token |
| ✅ 好 chunk：同义重复       | 需要审批 / 必须走审批流程 / 必须发起审批                     | 是           | 同一主判断下自然同义表达，检索仍友好          |


### 检索
检索要设置最低相似度，反馈top-k，不要只返回一条。没有检索到结果的情况下，如果严格就不要调用大模型，避免胡编乱造。
知识chunk和问题，变为向量时，其方向就是表示语义，Embedding向量化时，会将语义近似朝一个方向。因此在检索时，余弦相似度对方向的匹配，才是语义相似度的匹配。

### 知识和向量库
知识需要经过Embedding，才能写入向量数据库。
不同厂商提供的Embedding接口能力，计算出的结果都是不同的。因此一套体系下，不能混用，如果要换模型(包括同一个模型的不同版本)，那么之前的向量全部需要重算。
以ES作为向量库为例，一般要一套索引一个向量计算方式，如果需要切换向量计算体系，换一个索引就行。这个需要提前规划。
